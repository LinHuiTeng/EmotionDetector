{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Building an Emotion Detector with LittleVGG_calvin(2).ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WtUCq3fi8coj","colab_type":"text"},"source":["# Using VGG for Emotion Detection"]},{"cell_type":"markdown","metadata":{"id":"I2OXSAba8col","colab_type":"text"},"source":["### Training Emotion Detector"]},{"cell_type":"code","metadata":{"id":"NVmy44vR8col","colab_type":"code","outputId":"a6086d80-afa1-4b91-ed11-d4560d0f4bc7","executionInfo":{"status":"ok","timestamp":1591678405374,"user_tz":-480,"elapsed":37063,"user":{"displayName":"林暉暉","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Give03rXbLX4fMG7EFRbuTOgXF_k8f0GIFo73ybjg=s64","userId":"11268380641282905806"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["from __future__ import print_function\n","import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')\n","os.getcwd()\n","num_classes = 2\n","img_rows, img_cols = 48, 48\n","batch_size = 16\n","\n","train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/Himirror_dataset/binarydata/train'\n","validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/Himirror_dataset/binarydata/valid'\n","\n","# Let's use some data augmentaiton \n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      rotation_range=30,\n","      shear_range=0.3,\n","      zoom_range=0.3,\n","      width_shift_range=0.4,\n","      height_shift_range=0.4,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n"," \n","validation_datagen = ImageDataGenerator(rescale=1./255)\n"," \n","train_generator = train_datagen.flow_from_directory(\n","        train_data_dir,\n","        color_mode = 'grayscale',\n","        target_size=(img_rows, img_cols),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle=True)\n"," \n","validation_generator = validation_datagen.flow_from_directory(\n","        validation_data_dir,\n","        color_mode = 'grayscale',\n","        target_size=(img_rows, img_cols),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","Found 2000 images belonging to 2 classes.\n","Found 600 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kA94d8AH8coq","colab_type":"text"},"source":["## Our Keras Imports"]},{"cell_type":"code","metadata":{"id":"YTXFguyq8cor","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9t4crHe8cou","colab_type":"text"},"source":["## Keras LittleVGG Model"]},{"cell_type":"code","metadata":{"id":"sNvzRlMO8cov","colab_type":"code","outputId":"f0f0828d-e924-45f0-b99b-72a474bfd515","executionInfo":{"status":"ok","timestamp":1591678422847,"user_tz":-480,"elapsed":8964,"user":{"displayName":"林暉暉","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Give03rXbLX4fMG7EFRbuTOgXF_k8f0GIFo73ybjg=s64","userId":"11268380641282905806"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#drop_pram = 0.5\n","model = Sequential()\n","\n","model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n","                 input_shape = (img_rows, img_cols, 1)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n","                 input_shape = (img_rows, img_cols, 1)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","#model.add(Dropout(drop_pram))\n","\n","# Block #2: second CONV => RELU => CONV => RELU => POOL\n","# layer set\n","model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","#model.add(Dropout(drop_pram))\n","\n","# Block #3: third CONV => RELU => CONV => RELU => POOL\n","# layer set\n","model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","#model.add(Dropout(drop_pram))\n","\n","# Block #4: third CONV => RELU => CONV => RELU => POOL\n","# layer set\n","model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","#model.add(Dropout(drop_pram))\n","\n","# Block #5: first set of FC => RELU layers\n","model.add(Flatten())\n","model.add(Dense(64, kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","#model.add(Dropout(0.2))\n","\n","# Block #6: second set of FC => RELU layers\n","model.add(Dense(64, kernel_initializer=\"he_normal\"))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","#model.add(Dropout(0.2))\n","\n","# Block #7: softmax classifier\n","model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n","#model.add(Activation(\"softmax\"))\n","model.add(Activation(\"sigmoid\"))\n","\n","print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 48, 48, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 48, 48, 32)        128       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 48, 48, 32)        0         \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 24, 24, 64)        256       \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 6, 6, 256)         295168    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","activation_7 (Activation)    (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 6, 6, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 2304)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                147520    \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 64)                256       \n","_________________________________________________________________\n","activation_9 (Activation)    (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 64)                256       \n","_________________________________________________________________\n","activation_10 (Activation)   (None, 64)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 2)                 130       \n","_________________________________________________________________\n","activation_11 (Activation)   (None, 2)                 0         \n","=================================================================\n","Total params: 1,327,842\n","Trainable params: 1,325,666\n","Non-trainable params: 2,176\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dTyr5fEZ8coy","colab_type":"text"},"source":["## Training our model"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"62M4aw_Y8coy","colab_type":"code","outputId":"b67faf5a-5a7c-4b0c-c6c1-38b5b04f2055","executionInfo":{"status":"ok","timestamp":1591075527809,"user_tz":-480,"elapsed":188564,"user":{"displayName":"林暉暉","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Give03rXbLX4fMG7EFRbuTOgXF_k8f0GIFo73ybjg=s64","userId":"11268380641282905806"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from keras.optimizers import RMSprop, SGD, Adam\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","                     \n","checkpoint = ModelCheckpoint(\"/content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\",\n","                             monitor=\"val_accuracy\",\n","                             mode=\"max\",\n","                             save_best_only = True,\n","                             verbose=1)\n","\n","patience_num = 5\n","earlystop = EarlyStopping(monitor = 'val_accuracy', \n","                          min_delta = 0, \n","                          patience = patience_num,\n","                          verbose = 1,\n","                          restore_best_weights = True)\n","\n","reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.2, patience = patience_num - 2, verbose = 1, min_delta = 0.0001)\n","\n","# we put our call backs into a callback list\n","callbacks = [earlystop, checkpoint, reduce_lr]\n","# We use a very small learning rate \n","#model.compile(loss = 'categorical_crossentropy',\n","model.compile(loss = 'binary_crossentropy',\n","              optimizer = Adam(lr=0.001),\n","              metrics = ['accuracy'])\n","\n","nb_train_samples = 2000\n","nb_validation_samples = 600\n","epochs = 50\n","\n","history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch = nb_train_samples // batch_size,\n","    epochs = epochs,\n","    callbacks = callbacks,\n","    validation_data = validation_generator,\n","    validation_steps = nb_validation_samples // batch_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","125/125 [==============================] - 7s 56ms/step - loss: 0.6548 - accuracy: 0.6166 - val_loss: 2.4166 - val_accuracy: 0.5507\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.55068, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 2/50\n","125/125 [==============================] - 5s 44ms/step - loss: 0.6408 - accuracy: 0.6355 - val_loss: 0.4545 - val_accuracy: 0.7217\n","\n","Epoch 00002: val_accuracy improved from 0.55068 to 0.72175, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 3/50\n","125/125 [==============================] - 5s 43ms/step - loss: 0.6161 - accuracy: 0.6619 - val_loss: 0.2510 - val_accuracy: 0.7149\n","\n","Epoch 00003: val_accuracy did not improve from 0.72175\n","Epoch 4/50\n","125/125 [==============================] - 5s 44ms/step - loss: 0.6093 - accuracy: 0.6621 - val_loss: 0.5622 - val_accuracy: 0.7688\n","\n","Epoch 00004: val_accuracy improved from 0.72175 to 0.76884, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 5/50\n","125/125 [==============================] - 5s 44ms/step - loss: 0.5985 - accuracy: 0.6626 - val_loss: 0.2045 - val_accuracy: 0.7029\n","\n","Epoch 00005: val_accuracy did not improve from 0.76884\n","Epoch 6/50\n","125/125 [==============================] - 5s 44ms/step - loss: 0.5634 - accuracy: 0.7025 - val_loss: 0.0581 - val_accuracy: 0.5916\n","\n","Epoch 00006: val_accuracy did not improve from 0.76884\n","Epoch 7/50\n","125/125 [==============================] - 5s 44ms/step - loss: 0.5665 - accuracy: 0.7040 - val_loss: 0.6581 - val_accuracy: 0.7979\n","\n","Epoch 00007: val_accuracy improved from 0.76884 to 0.79795, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 8/50\n","125/125 [==============================] - 5s 44ms/step - loss: 0.5506 - accuracy: 0.7180 - val_loss: 0.7010 - val_accuracy: 0.7997\n","\n","Epoch 00008: val_accuracy improved from 0.79795 to 0.79966, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 9/50\n","125/125 [==============================] - 5s 43ms/step - loss: 0.5120 - accuracy: 0.7452 - val_loss: 0.0395 - val_accuracy: 0.8022\n","\n","Epoch 00009: val_accuracy improved from 0.79966 to 0.80223, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 10/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.5247 - accuracy: 0.7418 - val_loss: 0.3878 - val_accuracy: 0.8219\n","\n","Epoch 00010: val_accuracy improved from 0.80223 to 0.82192, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 11/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.4993 - accuracy: 0.7501 - val_loss: 0.1865 - val_accuracy: 0.7688\n","\n","Epoch 00011: val_accuracy did not improve from 0.82192\n","Epoch 12/50\n","125/125 [==============================] - 5s 43ms/step - loss: 0.5132 - accuracy: 0.7559 - val_loss: 0.3441 - val_accuracy: 0.8271\n","\n","Epoch 00012: val_accuracy improved from 0.82192 to 0.82705, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 13/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.4324 - accuracy: 0.7979 - val_loss: 0.4115 - val_accuracy: 0.8485\n","\n","Epoch 00013: val_accuracy improved from 0.82705 to 0.84846, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 14/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.4647 - accuracy: 0.7775 - val_loss: 0.2840 - val_accuracy: 0.8322\n","\n","Epoch 00014: val_accuracy did not improve from 0.84846\n","Epoch 15/50\n","125/125 [==============================] - 5s 43ms/step - loss: 0.4389 - accuracy: 0.7906 - val_loss: 0.0559 - val_accuracy: 0.7765\n","\n","Epoch 00015: val_accuracy did not improve from 0.84846\n","Epoch 16/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.4498 - accuracy: 0.7924 - val_loss: 0.1888 - val_accuracy: 0.8545\n","\n","Epoch 00016: val_accuracy improved from 0.84846 to 0.85445, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 17/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.4471 - accuracy: 0.7979 - val_loss: 0.2181 - val_accuracy: 0.8639\n","\n","Epoch 00017: val_accuracy improved from 0.85445 to 0.86387, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 18/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.4315 - accuracy: 0.8074 - val_loss: 0.1561 - val_accuracy: 0.8450\n","\n","Epoch 00018: val_accuracy did not improve from 0.86387\n","Epoch 19/50\n","125/125 [==============================] - 5s 41ms/step - loss: 0.4351 - accuracy: 0.8029 - val_loss: 0.6450 - val_accuracy: 0.8134\n","\n","Epoch 00019: val_accuracy did not improve from 0.86387\n","Epoch 20/50\n","125/125 [==============================] - 5s 41ms/step - loss: 0.4252 - accuracy: 0.8064 - val_loss: 0.1360 - val_accuracy: 0.8579\n","\n","Epoch 00020: val_accuracy did not improve from 0.86387\n","\n","Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","Epoch 21/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3693 - accuracy: 0.8338 - val_loss: 0.4471 - val_accuracy: 0.8818\n","\n","Epoch 00021: val_accuracy improved from 0.86387 to 0.88185, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 22/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3916 - accuracy: 0.8170 - val_loss: 0.2860 - val_accuracy: 0.8878\n","\n","Epoch 00022: val_accuracy improved from 0.88185 to 0.88784, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 23/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3721 - accuracy: 0.8343 - val_loss: 0.2724 - val_accuracy: 0.8776\n","\n","Epoch 00023: val_accuracy did not improve from 0.88784\n","Epoch 24/50\n","125/125 [==============================] - 5s 43ms/step - loss: 0.3651 - accuracy: 0.8366 - val_loss: 0.4551 - val_accuracy: 0.8878\n","\n","Epoch 00024: val_accuracy did not improve from 0.88784\n","Epoch 25/50\n","125/125 [==============================] - 5s 41ms/step - loss: 0.3558 - accuracy: 0.8507 - val_loss: 0.2498 - val_accuracy: 0.8913\n","\n","Epoch 00025: val_accuracy improved from 0.88784 to 0.89127, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 26/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3437 - accuracy: 0.8537 - val_loss: 0.1137 - val_accuracy: 0.9050\n","\n","Epoch 00026: val_accuracy improved from 0.89127 to 0.90497, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 27/50\n","125/125 [==============================] - 5s 41ms/step - loss: 0.3314 - accuracy: 0.8655 - val_loss: 0.7192 - val_accuracy: 0.8801\n","\n","Epoch 00027: val_accuracy did not improve from 0.90497\n","Epoch 28/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3578 - accuracy: 0.8399 - val_loss: 0.2946 - val_accuracy: 0.9075\n","\n","Epoch 00028: val_accuracy improved from 0.90497 to 0.90753, saving model to /content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\n","Epoch 29/50\n","125/125 [==============================] - 5s 41ms/step - loss: 0.3462 - accuracy: 0.8350 - val_loss: 0.2774 - val_accuracy: 0.8827\n","\n","Epoch 00029: val_accuracy did not improve from 0.90753\n","Epoch 30/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3374 - accuracy: 0.8617 - val_loss: 0.4629 - val_accuracy: 0.8810\n","\n","Epoch 00030: val_accuracy did not improve from 0.90753\n","Epoch 31/50\n","125/125 [==============================] - 5s 43ms/step - loss: 0.3382 - accuracy: 0.8537 - val_loss: 0.1400 - val_accuracy: 0.9015\n","\n","Epoch 00031: val_accuracy did not improve from 0.90753\n","\n","Epoch 00031: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","Epoch 32/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3289 - accuracy: 0.8557 - val_loss: 0.2094 - val_accuracy: 0.8973\n","\n","Epoch 00032: val_accuracy did not improve from 0.90753\n","Epoch 33/50\n","125/125 [==============================] - 5s 42ms/step - loss: 0.3473 - accuracy: 0.8550 - val_loss: 0.5806 - val_accuracy: 0.9015\n","Restoring model weights from the end of the best epoch\n","\n","Epoch 00033: val_accuracy did not improve from 0.90753\n","Epoch 00033: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ii45u2Pd8co2","colab_type":"text"},"source":["### Loading our saved model"]},{"cell_type":"code","metadata":{"id":"0RbAU2sp8co2","colab_type":"code","colab":{}},"source":["from keras.models import load_model\n","import tensorflow as tf\n","classifier = load_model('/content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5')\n","#new_model= tf.keras.models.load_model(filepath=\"/content/gdrive/My Drive/Colab Notebooks/emotion_little_vgg_cal_binary.h5\")\n","#tflite_converter = tf.lite.TFLiteConverter.from_keras_model(new_model)\n","#tflite_model = tflite_converter.convert()\n","#open(\"/content/gdrive/My Drive/Colab Notebooks/emotion_binary.tflite\", \"wb\").write(tflite_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZnoHvDL8co5","colab_type":"code","outputId":"e03d023f-8a11-4b1e-980e-6f6f762037f1","executionInfo":{"status":"ok","timestamp":1591678440523,"user_tz":-480,"elapsed":1329,"user":{"displayName":"林暉暉","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Give03rXbLX4fMG7EFRbuTOgXF_k8f0GIFo73ybjg=s64","userId":"11268380641282905806"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np\n","\n","nb_validation_samples = 600\n","# We need to recreate our validation generator with shuffle = false\n","validation_generator = validation_datagen.flow_from_directory(\n","        validation_data_dir,\n","        color_mode = 'grayscale',\n","        target_size=(img_rows, img_cols),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle=False)\n","\n","class_labels = validation_generator.class_indices\n","class_labels = {v: k for k, v in class_labels.items()}\n","classes = list(class_labels.values())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 600 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Fe-sxeDBox-","colab_type":"code","outputId":"91e5e4a6-7038-409d-d4ea-978f4cd58dbf","executionInfo":{"status":"ok","timestamp":1591605483078,"user_tz":-480,"elapsed":54507,"user":{"displayName":"林暉暉","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Give03rXbLX4fMG7EFRbuTOgXF_k8f0GIFo73ybjg=s64","userId":"11268380641282905806"}},"colab":{"base_uri":"https://localhost:8080/","height":700}},"source":["#Confution Matrix and Classification Report\n","Y_pred = classifier.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n","y_pred = np.argmax(Y_pred, axis=1)\n","\n","print('Confusion Matrix')\n","print(confusion_matrix(validation_generator.classes, y_pred))\n","print('Classification Report')\n","target_names = list(class_labels.values())\n","print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n","\n","plt.figure(figsize=(8,8))\n","cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n","\n","plt.imshow(cnf_matrix, interpolation='nearest')\n","plt.colorbar()\n","tick_marks = np.arange(len(classes))\n","_ = plt.xticks(tick_marks, classes, rotation=90)\n","_ = plt.yticks(tick_marks, classes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Confusion Matrix\n","[[259  41]\n"," [ 16 284]]\n","Classification Report\n","              precision    recall  f1-score   support\n","\n","       Happy       0.94      0.86      0.90       300\n","     Unhappy       0.87      0.95      0.91       300\n","\n","    accuracy                           0.91       600\n","   macro avg       0.91      0.91      0.90       600\n","weighted avg       0.91      0.91      0.90       600\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"qNeAPI3V8co9","colab_type":"code","outputId":"f64495fe-ba79-449f-f02b-a5251929bbfb","executionInfo":{"status":"ok","timestamp":1591681772753,"user_tz":-480,"elapsed":1789,"user":{"displayName":"林暉暉","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Give03rXbLX4fMG7EFRbuTOgXF_k8f0GIFo73ybjg=s64","userId":"11268380641282905806"}},"colab":{"base_uri":"https://localhost:8080/","height":167}},"source":["from keras.models import load_model\n","from keras.preprocessing import image\n","import numpy as np\n","import os\n","import cv2\n","import numpy as np\n","from os import listdir\n","from os.path import isfile, join\n","import re\n","from IPython.display import Image\n","\n","def getRandomImage(path, img_width, img_height):\n","    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n","    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n","    random_directory = np.random.randint(0,len(folders))\n","    path_class = folders[random_directory]\n","    file_path = path + path_class\n","    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n","    random_file_index = np.random.randint(0,len(file_names))\n","    image_name = file_names[random_file_index]\n","    final_path = file_path + \"/\" + image_name\n","    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n","\n","# dimensions of our images\n","img_width, img_height = 48, 48\n","\n","files = []\n","predictions = []\n","true_labels = []\n","\n","# predicting images\n","for i in range(0, 1):\n","    path = \"/content/gdrive/My Drive/Colab Notebooks/Himirror_dataset/binarydata/valid/\"  \n","    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n","    files.append(final_path)\n","    true_labels.append(true_label)\n","    x = image.img_to_array(img)\n","    x = x * 1./255\n","    x = np.expand_dims(x, axis=0)\n","    images = np.vstack([x])\n","    classes = classifier.predict_classes(images)\n","    predictions.append(classes)\n","    print(\"File Name : \" + final_path)\n","    print(\"Predict : \" + class_labels[predictions[i][0]])\n","    print(\"True : \" + true_labels[i])\n","    print(classifier.predict(images))    \n","Image(filename = final_path )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n","  warnings.warn('grayscale is deprecated. Please use '\n"],"name":"stderr"},{"output_type":"stream","text":["File Name : /content/gdrive/My Drive/Colab Notebooks/Himirror_dataset/binarydata/valid/Happy/7047.jpg\n","Predict : Happy\n","True : Happy\n","[[0.95698315 0.04850028]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"image/jpeg":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":34}]}]}